# ğŸŒ™ Agent Deen | ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¯ÙŠÙ†

**Trilingual AI Shariah Chatbot for Islamic Finance**

âœ¨ Ask questions in **Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)**, **English**, or **Bahasa Melayu**

Powered by **Ollama** (Free & Local) or **Claude Haiku** (High Quality) with RAG from authoritative Shariah sources.

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| **Python** | 3.11+ | Required |
| **Ollama** | Latest | For local LLM inference |
| **Pinecone** | Free tier | For vector database |

### 1. Clone & Setup Environment

```bash
# Clone the repository
git clone <repository-url>
cd for-ummah

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On macOS/Linux
# OR
.\venv\Scripts\activate   # On Windows
```

### 2. Install Dependencies

```bash
# Install Python packages
pip install -r requirements.txt
```

**Dependencies include:**

- `fastapi`, `uvicorn` - Backend API
- `streamlit` - Web UI
- `pinecone` - Vector database
- `pymupdf` - PDF text extraction
- `playwright` - Web scraping with WAF bypass
- `requests`, `beautifulsoup4` - Web scraping

### 3. Install Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows - Download from https://ollama.com
```

**Pull required models:**

```bash
# Start Ollama service
ollama serve

# In another terminal, pull models
ollama pull llama3.2           # Main LLM for chat
ollama pull nomic-embed-text   # Embeddings for RAG
```

### 4. Configure Environment

```bash
# Copy example config
cp .env.example .env

# Edit .env with your Pinecone API key
```

**Required `.env` variables:**

```env
# Pinecone (required for vector DB)
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX=shariah-kb

# Optional Auth (Required only for Claude)
ANTHROPIC_API_KEY=sk-ant-api03-...

# Optional settings
DATA_DIR=data
LOG_LEVEL=INFO

# Optional RAG tuning
RAG_RELEVANCE_THRESHOLD=0.65  # Min relevance score (0.60-0.70)
```

> **Note:** Ollama runs 100% locally for free. An Anthropic API key is only needed if you want to use the Claude Haiku model.

### 5. Run the Application

**Terminal 1 - Ollama (must be running):**

```bash
ollama serve
```

**Terminal 2 - API Backend:**

```bash
uvicorn src.api.main:app --reload --port 8000
```

**Terminal 3 - Streamlit UI:**

```bash
streamlit run app.py
```

**Access:**

- ğŸŒ **Streamlit UI:** <http://localhost:8501>
- ğŸ”§ **API Docs:** <http://localhost:8000/docs>

---

## ğŸ“ Project Structure

```
for-ummah/
â”œâ”€â”€ app.py                  # Streamlit web UI
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment template
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # Configuration, language detection
â”‚   â”œâ”€â”€ scrapers/          # Web scrapers (BNM, AAOIFI, JAKIM)
â”‚   â”œâ”€â”€ processors/        # PDF extraction, text chunking
â”‚   â”œâ”€â”€ vector_db/         # Pinecone + Ollama embeddings
â”‚   â”œâ”€â”€ ai/                # RAG pipeline, prompts, Ollama + Claude LLMs
â”‚   â”œâ”€â”€ services/          # ChatService orchestrator
â”‚   â”‚   â”œâ”€â”€ chat.py        # ChatService orchestrator
â”‚   â”‚   â”œâ”€â”€ history.py     # Chat history persistence
â”‚   â”‚   â””â”€â”€ ingestion.py   # Document ingestion pipeline
â”‚   â””â”€â”€ api/               # FastAPI endpoints
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ reindex_with_pages.py  # Re-process PDFs with page tracking
â”‚   â”œâ”€â”€ scrape_url.py          # Download & index PDF from URL
â”‚   â””â”€â”€ translate_claude.py    # (Optional) Batch translation tool
â”‚
â””â”€â”€ data/                  # Shariah documents (PDFs)
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Cost |
|-----------|------------|------|
| **LLM (Local)** | Ollama llama3.2 | FREE (local) |
| **LLM (Cloud)** | Claude 3.5 Haiku | ~$0.001/query (High Quality) |
| **Embeddings** | Ollama nomic-embed-text | FREE (local) |
| **Vector DB** | Pinecone | Free tier |
| **Backend** | FastAPI | - |
| **Frontend** | Streamlit | - |
| **PDF Extraction** | PyMuPDF â†’ Tesseract OCR (cascade) | FREE |

---

## âœ¨ Features

- ğŸŒ **Trilingual:** Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©), English, Bahasa Melayu
- ğŸ“š **Authoritative Sources:** BNM, AAOIFI, SC Malaysia, JAKIM
- ğŸ¤– **Hybrid AI:** Choose between **Ollama (Free)** or **Claude Haiku (Smart)**
- ğŸ”„ **Query Translation:** Auto-translates Malay/Arabic queries to English for better search precision
- ğŸ“„ **Smart PDF:** Page-level tracking with Arabic OCR support
- ğŸ” **Source Citations:** Every answer shows its source page numbers
- ğŸ’¬ **Chat History:** Persistent conversation sessions with sidebar navigation
- ğŸ“¤ **Source Management:** Upload PDFs or add by URL directly in UI
- ğŸ”— **PDF Viewer:** Click source citations to open PDF at exact page

---

## ğŸ“¦ Indexing Documents

### Re-index all PDFs

```bash
# Process all PDFs and index with page tracking
python scripts/reindex_with_pages.py
```

### Add a single PDF from URL

```bash
# Download and index a PDF directly from URL
python scripts/scrape_url.py "https://example.com/document.pdf"

# With custom title and source
python scripts/scrape_url.py "URL" --title "Custom Title" --source BNM
```

This will:

1. Extract text from PDFs with sentence-based chunking
2. Preserve page numbers for source citations
3. Upload to Pinecone with metadata

---

## ğŸ§ª API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/chat` | POST | Main chat endpoint |
| `/health` | GET | Health check |
| `/docs` | GET | Swagger documentation |
| `/history/chats` | GET | List all chat sessions |
| `/history/chat/{id}` | GET | Get specific chat session |
| `/history/chat` | POST | Create/update chat session |
| `/ingest/url` | POST | Ingest document from URL |
| `/ingest/upload` | POST | Upload and ingest PDF |
| `/pdf/{source}/{filename}` | GET | Serve PDF file |

**Example API call:**

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is Murabaha?", 
    "language": "en",
    "model": "claude" 
  }'
```

---

## ğŸ”§ Troubleshooting

### Ollama not connecting

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
ollama serve
```

### Pinecone connection issues

- Verify your API key in `.env`
- Check index name matches `PINECONE_INDEX`
- Ensure index exists in Pinecone dashboard

### PDF extraction problems

- Digital PDFs: Handled by PyMuPDF
- Scanned PDFs: Requires Tesseract OCR

```bash
# Install Tesseract (optional for scanned PDFs)
# macOS
brew install tesseract tesseract-lang

# Ubuntu
sudo apt install tesseract-ocr tesseract-ocr-ara
```

---

## ğŸ“„ License

Built for the Ummah ğŸŒ™

---

## ğŸ“ Support

For questions or contributions, please open an issue on GitHub.
