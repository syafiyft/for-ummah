# ğŸŒ™ Agent Deen | ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¯ÙŠÙ†

**Trilingual AI Shariah Chatbot for Islamic Finance**

âœ¨ Ask questions in **Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)**, **English**, or **Bahasa Melayu**

Powered by **Ollama** (Free & Local) or **Claude Haiku** (High Quality) with RAG from authoritative Shariah sources.

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| **Python** | 3.11+ | Required |
| **Ollama** | Latest | For local LLM inference |
| **Pinecone** | Free tier | For vector database |
| **Supabase** | Free tier | For PostgreSQL + Storage |

### 1. Clone & Setup Environment

```bash
# Clone the repository
git clone <repository-url>
cd for-ummah

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On macOS/Linux
# OR
.\venv\Scripts\activate   # On Windows
```

### 2. Install Dependencies

```bash
# Install Python packages
pip install -r requirements.txt
```

**Dependencies include:**

- `fastapi`, `uvicorn` - Backend API
- `streamlit` - Web UI
- `pinecone` - Vector database
- `pymupdf` - PDF text extraction
- `playwright` - Web scraping with WAF bypass
- `requests`, `beautifulsoup4` - Web scraping

### 3. Install Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows - Download from https://ollama.com
```

**Pull required models:**

```bash
# Start Ollama service
ollama serve

# In another terminal, pull models
ollama pull llama3.2           # Main LLM for chat
ollama pull nomic-embed-text   # Embeddings for RAG
```

### 4. Configure Environment

```bash
# Copy example config
cp .env.example .env

# Edit .env with your Pinecone API key
```

**Required `.env` variables:**

```env
# Pinecone (required for vector DB)
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX=shariah-kb

# Supabase (required for database + storage)
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_KEY=your-supabase-key

# Optional Auth (Required only for Claude)
ANTHROPIC_API_KEY=sk-ant-api03-...

# Optional settings
DATA_DIR=data
LOG_LEVEL=INFO

# Optional RAG tuning
RAG_RELEVANCE_THRESHOLD=0.60  # Min relevance score (0.60-0.70)
RAG_RERANK_TOP_K=25           # After reranking
```

> **Note:** Ollama runs 100% locally for free. An Anthropic API key is only needed if you want to use the Claude Haiku model.

### 5. Run the Application

**Terminal 1 - Ollama (must be running):**

```bash
ollama serve
```

**Terminal 2 - API Backend:**

```bash
uvicorn src.api.main:app --reload --port 8000
```

**Terminal 3 - Streamlit UI:**

```bash
streamlit run app.py
```

**Access:**

- ğŸŒ **Streamlit UI:** <http://localhost:8501>
- ğŸ”§ **API Docs:** <http://localhost:8000/docs>

---

## ğŸ“ Project Structure

```
for-ummah/
â”œâ”€â”€ app.py                  # Streamlit web UI
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment template
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # Configuration, language detection
â”‚   â”œâ”€â”€ db/                # Supabase integration
â”‚   â”‚   â”œâ”€â”€ client.py      # Supabase client singleton
â”‚   â”‚   â”œâ”€â”€ models.py      # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ storage.py     # Storage service (PDFs)
â”‚   â”‚   â””â”€â”€ repositories/  # Database repositories
â”‚   â”‚       â”œâ”€â”€ documents.py   # Document CRUD
â”‚   â”‚       â”œâ”€â”€ chat.py        # Chat sessions/messages
â”‚   â”‚       â”œâ”€â”€ ingestion.py   # Ingestion history
â”‚   â”‚       â””â”€â”€ job_status.py  # Background job status
â”‚   â”œâ”€â”€ scrapers/          # Web scrapers (BNM, SC Malaysia)
â”‚   â”œâ”€â”€ processors/        # PDF extraction, text chunking
â”‚   â”œâ”€â”€ vector_db/         # Pinecone + Ollama embeddings
â”‚   â”œâ”€â”€ ai/                # RAG pipeline, prompts, Ollama + Claude LLMs
â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”œâ”€â”€ chat.py        # ChatService orchestrator
â”‚   â”‚   â”œâ”€â”€ history.py     # Chat history (Supabase)
â”‚   â”‚   â””â”€â”€ ingestion.py   # Document ingestion pipeline
â”‚   â””â”€â”€ api/               # FastAPI endpoints
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ reindex_with_pages.py  # Re-process PDFs with page tracking
â”‚   â”œâ”€â”€ scrape_url.py          # Download & index PDF from URL
â”‚   â””â”€â”€ translate_claude.py    # (Optional) Batch translation tool
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.md    # System architecture diagrams
â”‚
â””â”€â”€ data/                  # Local cache (primary storage in Supabase)
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Cost |
|-----------|------------|------|
| **LLM (Local)** | Ollama llama3.2 | FREE (local) |
| **LLM (Cloud)** | Claude 3.5 Haiku | ~$0.001/query (High Quality) |
| **Embeddings** | Ollama nomic-embed-text | FREE (local) |
| **Vector DB** | Pinecone | Free tier |
| **Database** | Supabase PostgreSQL | Free tier |
| **Storage** | Supabase Storage | Free tier |
| **Backend** | FastAPI | - |
| **Frontend** | Streamlit | - |
| **PDF Extraction** | PyMuPDF â†’ Tesseract OCR (cascade) | FREE |
| **Reranking** | CrossEncoder (ms-marco-MiniLM) | FREE |

---

## âœ¨ Features

- ğŸŒ **Trilingual:** Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©), English, Bahasa Melayu
- ğŸ“š **Authoritative Sources:** BNM, AAOIFI, SC Malaysia, JAKIM
- ğŸ¤– **Hybrid AI:** Choose between **Ollama (Free)** or **Claude Haiku (Smart)**
- ğŸ¯ **High-Precision Reranking:** CrossEncoder model boosts search relevance
- ğŸ”„ **Query Translation:** Auto-translates Malay/Arabic queries to English for better search precision
- ğŸ“„ **Smart PDF:** Page-level tracking with Arabic OCR support
- ğŸ” **Source Verification:** Clickable citations with **Exact Quote**, **Page Previews (Image)**, & **Highlighted Text**
- ğŸ’¬ **Chat History:** Persistent conversation sessions stored in Supabase
- ğŸ“¤ **Source Management:** Upload PDFs or add by URL directly in UI
- ğŸ¤– **Automated Updates:** Scheduled background scraper (APScheduler) checks for new BNM/SC documents
- ğŸ“Š **Admin Dashboard:** Monitor document counts, storage, system health, and trigger manual updates
- â˜ï¸ **Cloud Storage:** All documents stored in Supabase Storage with secure access

---

## ğŸ“¦ Indexing Documents

### Re-index all PDFs

```bash
# Process all PDFs and index with page tracking
python scripts/reindex_with_pages.py
```

### Add a single PDF from URL

```bash
# Download and index a PDF directly from URL
python scripts/scrape_url.py "https://example.com/document.pdf"

# With custom title and source
python scripts/scrape_url.py "URL" --title "Custom Title" --source BNM
```

This will:

1. Extract text from PDFs with sentence-based chunking
2. Preserve page numbers for source citations
3. Upload to Pinecone with metadata

---

## ğŸ§ª API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/chat` | POST | Main chat endpoint |
| `/health` | GET | Health check |
| `/docs` | GET | Swagger documentation |
| `/history/chats` | GET | List all chat sessions |
| `/history/chat/{id}` | GET | Get specific chat session |
| `/history/chat` | POST | Create/update chat session |
| `/history/sources` | GET | List all indexed sources |
| `/ingest/url` | POST | Ingest document from URL |
| `/ingest/upload` | POST | Upload and ingest PDF |
| `/pdf/{source}/{filename}` | GET | Serve PDF file |
| `/pdf/list` | GET | List available PDFs |
| `/admin/trigger-update` | POST | Trigger scraper update |
| `/admin/job-status` | GET | Get background job status |

**Example API call:**

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is Murabaha?", 
    "language": "en",
    "model": "claude" 
  }'
```

---

## ğŸ”§ Troubleshooting

### Ollama not connecting

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
ollama serve
```

### Pinecone connection issues

- Verify your API key in `.env`
- Check index name matches `PINECONE_INDEX`
- Ensure index exists in Pinecone dashboard

### PDF extraction problems

- Digital PDFs: Handled by PyMuPDF
- Scanned PDFs: Requires Tesseract OCR

```bash
# Install Tesseract (optional for scanned PDFs)
# macOS
brew install tesseract tesseract-lang

# Ubuntu
sudo apt install tesseract-ocr tesseract-ocr-ara
```

---

## ğŸ“„ License

Built for the Ummah ğŸŒ™

---

## ğŸ“ Support

For questions or contributions, please open an issue on GitHub.
